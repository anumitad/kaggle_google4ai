{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# SET UP ","metadata":{}},{"cell_type":"code","source":"import glob\nimport json\nimport os\nfrom typing import Optional, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport transformers\nfrom transformers import AutoModel, AutoTokenizer\nfrom IPython.display import display\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import GroupKFold\nfrom tensorflow.keras.utils import plot_model\nfrom tqdm.notebook import tqdm\nfrom tqdm import tqdm\nfrom scipy import spatial\nfrom collections import OrderedDict\n\nimport re\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport string\nimport copy \nnltk.download('stopwords')\nnltk.download('omw-1.4')","metadata":{"execution":{"iopub.status.busy":"2022-09-01T03:05:50.047503Z","iopub.execute_input":"2022-09-01T03:05:50.047959Z","iopub.status.idle":"2022-09-01T03:05:59.342460Z","shell.execute_reply.started":"2022-09-01T03:05:50.047921Z","shell.execute_reply":"2022-09-01T03:05:59.341205Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"DATA_PATH = \"../input/AI4Code\"\nBASE_MODEL = 'bert-base-multilingual-cased'\nN_SPLITS = 5\nSEQ_LEN = 128\nRANDOM_STATE = 42\nLIMIT = 1000\n\ntry:\n    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(TPU)\n    tf.tpu.experimental.initialize_tpu_system(TPU)\n    STRATEGY = tf.distribute.experimental.TPUStrategy(TPU)\n    BATCH_SIZE = 128 * STRATEGY.num_replicas_in_sync\nexcept Exception:\n    TPU = None\n    STRATEGY = tf.distribute.get_strategy()\n    BATCH_SIZE = 32\n    #LIMIT = 10_000\n\nprint(\"TensorFlow\", tf.__version__)\n\nif TPU is not None:\n    print(\"Using TPU v3-8\")\nelse:\n    print(\"Using GPU/CPU\")\n\nprint(\"Batch size:\", BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2022-09-01T03:05:59.345211Z","iopub.execute_input":"2022-09-01T03:05:59.346254Z","iopub.status.idle":"2022-09-01T03:05:59.365646Z","shell.execute_reply.started":"2022-09-01T03:05:59.346206Z","shell.execute_reply":"2022-09-01T03:05:59.364479Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# HELPER FUNCTIONS","metadata":{}},{"cell_type":"code","source":"def read_notebook(path):\n    with open(path) as file:\n        df = pd.DataFrame(json.load(file))\n    df[\"id\"] = os.path.splitext(os.path.basename(path))[0]\n    return df\n\ndef clean_source(sources):\n    clean_source = copy.deepcopy(sources)\n    \n    for i, source in enumerate(tqdm(clean_source)):\n        source = source.lower()\n        source = re.sub(\"[^\\w\\s]\", \" \", source)\n        source = source.split()\n        \n        source_nostop = []\n        for word in source:\n            if word not in stopwords.words():\n                source_nostop.append(word)\n                \n        lemmatizer = WordNetLemmatizer()\n        source_lem = []\n        for word in source_nostop:\n            source_lem.append(lemmatizer.lemmatize(word))\n        \n        source = \" \".join(source_lem)\n        clean_source[i] = source\n    \n    return clean_source\n\ndef expand_order(row):\n    cell_ids = row[1].split(\" \")\n    df = pd.DataFrame(\n        {\n            \"id\": [row[0] for _ in range(len(cell_ids))],\n            \"cell_id\": cell_ids,\n            \"rank\": range(len(cell_ids)),\n        }\n    )\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-09-01T03:05:59.367911Z","iopub.execute_input":"2022-09-01T03:05:59.368486Z","iopub.status.idle":"2022-09-01T03:05:59.380227Z","shell.execute_reply.started":"2022-09-01T03:05:59.368439Z","shell.execute_reply":"2022-09-01T03:05:59.379224Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def get_input_ids(df, notebook_id, maxlen, tokenizer):\n    # get lists of code_ids, input_ids (from tokenizer), source - all separated based on code/markdown\n    df_temp = df.loc[df['id'] == notebook_id]\n    \n    code = df_temp[df_temp['cell_type'] == 'code']\n    code_ids = code['cell_id'].values.tolist() \n    code_sources = code['clean_source'].values.tolist()  \n    \n    code_encoded = tokenizer.batch_encode_plus(code_sources, add_special_tokens=True, \n                                               return_token_type_ids=True, truncation = True,\n                                               padding='max_length', max_length=maxlen)\n    \n    markdown = df_temp[df_temp['cell_type'] == 'markdown']\n    markdown_ids = markdown['cell_id'].values.tolist()   \n    markdown_sources = markdown['clean_source'].values.tolist()   \n    \n    markdown_encoded = tokenizer.batch_encode_plus(markdown_sources, add_special_tokens=True, \n                                               return_token_type_ids=True, truncation = True,\n                                               padding='max_length', max_length=maxlen)\n    \n    return (code_ids, code_encoded['input_ids'], code_sources, \n            markdown_ids, markdown_encoded['input_ids'], markdown_sources)\n\n\ndef jaccard_similarity(list1, list2):\n    intersection = len(list(set(list1).intersection(list2)))\n    union = (len(set(list1)) + len(set(list2))) - intersection\n    return float(intersection) / union\n\ndef max_similarity(nb_id, code_cell_ids, code_input_ids, mkdn_cell_ids, mkdn_input_ids):\n    # get markdown-code pair with the highest jaccard similarity (based on input_ids)\n    pairings = []\n    \n    for m_counter in range(len(mkdn_cell_ids)):\n        max_similarity = 0 \n        best_c_id = 0\n        current_m_id = mkdn_cell_ids[m_counter]\n        current_m_ii = mkdn_input_ids[m_counter]\n        \n        for c_counter in range(len(code_cell_ids)):\n            current_c_id = code_cell_ids[c_counter]\n            current_c_ii = code_input_ids[c_counter]\n            \n            temp_similarity = jaccard_similarity(current_m_ii, current_c_ii)\n            if temp_similarity > max_similarity:\n                max_similarity = temp_similarity \n                best_c_id = current_c_id\n            \n        pairings.append([current_m_id, best_c_id])\n    \n    return pairings","metadata":{"execution":{"iopub.status.busy":"2022-08-31T23:21:07.619038Z","iopub.execute_input":"2022-08-31T23:21:07.619409Z","iopub.status.idle":"2022-08-31T23:21:07.633237Z","shell.execute_reply.started":"2022-08-31T23:21:07.619373Z","shell.execute_reply":"2022-08-31T23:21:07.631802Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def get_closest_code_rank(m_rank, code_ranks):  \n    min_dis = 0 \n    closest_rank = 0 \n    \n    code_ranks_sorted = copy.deepcopy(code_ranks)\n    code_ranks_sorted.sort()\n\n    for i, c_rank in enumerate(code_ranks_sorted):\n        distance = abs(m_rank - c_rank)\n        prev_distance = abs(m_rank - code_ranks[i-1])\n        if i == 0:\n            min_dis = distance\n            closest_rank = c_rank\n        else:\n            if (distance < min_dis) and (c_rank < m_rank):\n                min_dis = distance\n                closest_rank = c_rank\n    \n    return closest_rank\n\ndef determine_labels(df, notebook_ids):\n    labels = []\n    for nb_id in tqdm(notebook_ids):\n        df_temp = df.loc[df['id'] == nb_id]\n        \n        # get ranks for code and markdown \n        code_ranks = df_temp.loc[df['cell_type'] == 'code']['rank'].values.tolist()\n        markdown_ranks = df_temp.loc[df['cell_type'] == 'markdown']['rank'].values.tolist()\n        \n        \n        # get cumcounts for code and markdown\n        code_count = df_temp.loc[df['cell_type'] == 'code']['cum_count'].values.tolist()\n        markdown_count = df_temp.loc[df['cell_type'] == 'markdown']['cum_count'].values.tolist()\n        \n        # get max cumcounts for code and markdown \n        max_code_count = max(code_count)\n        max_markdown_count = max(markdown_count)\n         \n        # calculate code labels \n        code_labels = [(count+1)/(max_code_count+2) for count in code_count]\n        code_rank_labels = {code_ranks[i]: code_labels[i] for i in range(len(code_ranks))}\n        \n        #print(code_rank_labels)\n        \n        # get min code labels \n        min_code_label = min(code_labels)\n        \n        # calculate markdown labels \n        markdown_labels = []\n        for m_rank in markdown_ranks:\n            closest_c_rank = get_closest_code_rank(m_rank, code_ranks)\n            base = (abs(m_rank - closest_c_rank))*(min_code_label/(max_markdown_count+2))\n\n            if m_rank == 0:\n                markdown_labels.append(base)\n                \n            else:\n                closest_code_label = code_rank_labels[closest_c_rank]\n                m_label = closest_code_label + base\n                markdown_labels.append(m_label)\n                \n        labels += code_labels \n        labels += markdown_labels \n        \n    df['labels'] = labels ","metadata":{"execution":{"iopub.status.busy":"2022-08-31T23:21:10.303312Z","iopub.execute_input":"2022-08-31T23:21:10.303661Z","iopub.status.idle":"2022-08-31T23:21:10.316935Z","shell.execute_reply.started":"2022-08-31T23:21:10.303636Z","shell.execute_reply":"2022-08-31T23:21:10.315601Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def get_inputs(df, notebook_ids, pairings, maxlen):   \n    markdown_count = []\n    code_count = []\n    code_label = []\n    base_label = []\n    \n    groups = []\n    \n    input_ids = []\n    attention_mask = []\n    token_type_ids = []\n    \n    for nb_id in tqdm(notebook_ids):\n        pairs = pairings[nb_id]\n        for pair in pairs:\n            # dataframe inputs\n            \n            markdown_id = pair[0]\n            code_id = pair[1]\n            \n            m_count = df.loc[df['cell_id'] == markdown_id]['cum_count'].values.tolist()\n            c_count = df.loc[df['cell_id'] == code_id]['cum_count'].values.tolist()\n            c_label = df.loc[df['cell_id'] == code_id]['labels'].values.tolist()\n            group = df.loc[df['cell_id'] == code_id]['ancestor_id'].values.tolist()\n            b_label = df.loc[(df['id'] == nb_id) & (df['cum_count'] == 0)]['labels'].values.tolist()\n            \n            markdown_count.append(m_count[0])\n            code_count.append(c_count[0])\n            code_label.append(c_label[0])\n            groups.append(group[0])\n            base_label.append(b_label[0])\n            \n            # bert inputs\n            \n            m_source = df.loc[df['cell_id'] == markdown_id]['clean_source'].values.tolist()\n            c_source = df.loc[df['cell_id'] == code_id]['clean_source'].values.tolist()\n            \n            encoding = tokenizer.encode_plus([m_source[0], c_source[0]], add_special_tokens=True, \n                                               return_token_type_ids=True, truncation = True,\n                                               padding='max_length', max_length=maxlen)\n            \n            input_ids.append(encoding['input_ids'])\n            attention_mask.append(encoding['attention_mask'])\n            token_type_ids.append(encoding['token_type_ids'])\n    \n    return (input_ids, attention_mask, token_type_ids, markdown_count, code_count, code_label, base_label, groups)\n\ndef get_labels(df, notebook_ids, pairings):\n    labels = []\n    \n    for nb_id in tqdm(notebook_ids):\n        pairs = pairings[nb_id]\n        for pair in pairs:\n            markdown_id = pair[0]\n            \n            temp_m_out = df.loc[df['cell_id'] == markdown_id]['labels'].values.tolist()\n            \n            labels.append(temp_m_out[0])\n    \n    return labels","metadata":{"execution":{"iopub.status.busy":"2022-08-31T23:21:13.289371Z","iopub.execute_input":"2022-08-31T23:21:13.289759Z","iopub.status.idle":"2022-08-31T23:21:13.304580Z","shell.execute_reply.started":"2022-08-31T23:21:13.289733Z","shell.execute_reply":"2022-08-31T23:21:13.303314Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def get_dataset(input_ids, attention_mask,  \n                markdown_count, code_count, code_label, base_label, \n                labels = None, ordered = False, repeated = False):\n    # input_ids, attention_mask, token_type_ids\n    # the label_rank for the first code cell in that notebook\n    # the label_rank for the code cell in the pairing\n    \n    if labels is not None:\n        dataset = tf.data.Dataset.from_tensor_slices(\n            ({\"input_ids\": input_ids, \"attention_mask\": attention_mask, \n              \"markdown_count\": markdown_count, \"code_count\": code_count, \n             \"code_label\": code_label, \"base_label\": base_label}, labels)\n        )\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices(\n            {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \n              \"markdown_count\": markdown_count, \"code_count\": code_count, \n             \"code_label\": code_label, \"base_label\": base_label}\n        )\n        \n    if repeated:\n        dataset = dataset.repeat()\n    if not ordered:\n        dataset = dataset.shuffle(1024)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2022-08-31T23:21:16.265087Z","iopub.execute_input":"2022-08-31T23:21:16.265386Z","iopub.status.idle":"2022-08-31T23:21:16.273289Z","shell.execute_reply.started":"2022-08-31T23:21:16.265361Z","shell.execute_reply":"2022-08-31T23:21:16.272095Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def get_model(): \n    backbone = transformers.TFDistilBertModel.from_pretrained(BASE_MODEL)\n    \n    input_ids = tf.keras.layers.Input(\n        shape=(SEQ_LEN,),\n        dtype=tf.int32,\n        name=\"input_ids\",\n    )\n    attention_mask = tf.keras.layers.Input(\n        shape=(SEQ_LEN,),\n        dtype=tf.int32,\n        name=\"attention_mask\",\n    )\n    \n    '''token_ids = tf.keras.layers.Input(\n        shape=(SEQ_LEN,),\n        dtype=tf.int32,\n        name=\"token_ids\",\n    )'''\n    \n    #input_ids, attention_mask, token_ids, markdown_count, code_count, code_label, base_label\n    \n    markdown_count = tf.keras.layers.Input(shape=(1, ), name=\"markdown_count\")\n    code_count = tf.keras.layers.Input(shape=(1, ), name=\"code_count\")\n    code_label = tf.keras.layers.Input(shape=(1, ), name=\"code_label\")\n    base_label = tf.keras.layers.Input(shape=(1, ), name=\"base_label\")\n    \n    \n    concat = tf.keras.layers.Concatenate()([markdown_count, code_count, code_label, base_label])\n    \n    x = backbone(\n        {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask\n        },\n    )\n    \n    x = tf.keras.layers.Dense(32, activation=\"linear\", dtype=\"float32\")(x[0][:, 0, :])\n    \n    y = tf.keras.layers.Dense(32)(concat)\n    \n    xy = tf.keras.layers.Concatenate()([x, y])\n    \n    outputs = tf.keras.layers.Dense(1, activation='linear', dtype='float32')(xy)\n\n    model = tf.keras.Model(\n        inputs=[input_ids, attention_mask, markdown_count, code_count, code_label, base_label],\n        outputs=outputs,\n    )\n    \n    model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n    loss=tf.keras.losses.MeanSquaredError(),\n    metrics = ['accuracy'] \n    )\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-08-31T23:21:18.942486Z","iopub.execute_input":"2022-08-31T23:21:18.942823Z","iopub.status.idle":"2022-08-31T23:21:18.955375Z","shell.execute_reply.started":"2022-08-31T23:21:18.942796Z","shell.execute_reply":"2022-08-31T23:21:18.954208Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# PREPROCESSING","metadata":{}},{"cell_type":"code","source":"paths = glob.glob(os.path.join(DATA_PATH, \"train\", \"*.json\"))\nif LIMIT is not None:\n    paths = paths[:LIMIT]\n\nsource_df = pd.concat([read_notebook(x) for x in tqdm(paths, total=len(paths))])\nsource_df = source_df.rename_axis('cell_id').reset_index()\n\norder_df = pd.read_csv(os.path.join(DATA_PATH, \"train_orders.csv\"), index_col=\"id\")\norder_df = pd.concat(\n    [expand_order(row) for row in tqdm(order_df.itertuples(), total=len(order_df))]\n)\n\n\nancestors_df = pd.read_csv(\n    os.path.join(DATA_PATH, \"train_ancestors.csv\"),\n    usecols=[\"id\", \"ancestor_id\"],\n    index_col=\"id\",\n)\n\ndf = source_df.merge(order_df, on=[\"id\", \"cell_id\"]).merge(ancestors_df, on='id')\ndf = df.dropna()\ndf","metadata":{"execution":{"iopub.status.busy":"2022-09-01T03:06:21.603463Z","iopub.execute_input":"2022-09-01T03:06:21.603876Z","iopub.status.idle":"2022-09-01T03:08:04.859344Z","shell.execute_reply.started":"2022-09-01T03:06:21.603839Z","shell.execute_reply":"2022-09-01T03:08:04.858077Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df_trial = df.copy(deep=True)\ndf_trial.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-01T03:08:04.861562Z","iopub.execute_input":"2022-09-01T03:08:04.862157Z","iopub.status.idle":"2022-09-01T03:08:04.878028Z","shell.execute_reply.started":"2022-09-01T03:08:04.862124Z","shell.execute_reply":"2022-09-01T03:08:04.876787Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"cleaned_source = clean_source(df_trial['source'])\ndf_trial['clean_source'] = cleaned_source","metadata":{"execution":{"iopub.status.busy":"2022-09-01T03:08:04.879341Z","iopub.execute_input":"2022-09-01T03:08:04.879665Z","iopub.status.idle":"2022-09-01T06:14:25.897500Z","shell.execute_reply.started":"2022-09-01T03:08:04.879636Z","shell.execute_reply":"2022-09-01T06:14:25.894725Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"df_trial.to_csv('cleaned_data_.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-09-01T06:14:28.238069Z","iopub.execute_input":"2022-09-01T06:14:28.239254Z","iopub.status.idle":"2022-09-01T06:14:28.951304Z","shell.execute_reply.started":"2022-09-01T06:14:28.239197Z","shell.execute_reply":"2022-09-01T06:14:28.949951Z"},"trusted":true},"execution_count":12,"outputs":[]}]}