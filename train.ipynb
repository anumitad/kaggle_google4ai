{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Set up","metadata":{}},{"cell_type":"code","source":"import glob\nimport json\nimport os\nfrom typing import Optional, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport transformers\nfrom transformers import AutoModel, AutoTokenizer\nfrom IPython.display import display\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import GroupKFold\nfrom tensorflow.keras.utils import plot_model\nfrom tqdm.notebook import tqdm\nfrom tqdm import tqdm\nfrom scipy import spatial\nfrom collections import OrderedDict\nfrom numpy import genfromtxt\n\nimport re\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport copy \nnltk.download('stopwords')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-02T05:41:12.629843Z","iopub.execute_input":"2022-09-02T05:41:12.630386Z","iopub.status.idle":"2022-09-02T05:41:21.323450Z","shell.execute_reply.started":"2022-09-02T05:41:12.630325Z","shell.execute_reply":"2022-09-02T05:41:21.322481Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"DATA_PATH = \"../input/AI4Code\"\nBASE_MODEL = '../input/huggingface-bert-variants/distilbert-base-multilingual-cased/distilbert-base-multilingual-cased'\nN_SPLITS = 5\nSEQ_LEN = 128\nRANDOM_STATE = 42\nLIMIT = 1000\n\ntry:\n    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(TPU)\n    tf.tpu.experimental.initialize_tpu_system(TPU)\n    STRATEGY = tf.distribute.experimental.TPUStrategy(TPU)\n    BATCH_SIZE = 128 * STRATEGY.num_replicas_in_sync\nexcept Exception:\n    TPU = None\n    STRATEGY = tf.distribute.get_strategy()\n    BATCH_SIZE = 32\n\nprint(\"TensorFlow\", tf.__version__)\n\nif TPU is not None:\n    print(\"Using TPU v3-8\")\nelse:\n    print(\"Using GPU/CPU\")\n\nprint(\"Batch size:\", BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2022-09-02T05:41:21.330907Z","iopub.execute_input":"2022-09-02T05:41:21.331195Z","iopub.status.idle":"2022-09-02T05:41:21.348381Z","shell.execute_reply.started":"2022-09-02T05:41:21.331169Z","shell.execute_reply":"2022-09-02T05:41:21.346773Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Helper functions ","metadata":{}},{"cell_type":"code","source":"def read_notebook(path):\n    with open(path) as file:\n        df = pd.DataFrame(json.load(file))\n    df[\"id\"] = os.path.splitext(os.path.basename(path))[0]\n    return df\n\ndef clean_source(sources):\n    clean_source = copy.deepcopy(sources)\n    \n    for i, source in enumerate(tqdm(clean_source)):\n        source = source.lower()\n        source = re.sub(\"[^\\w\\s]\", \" \", source)\n        source = source.split()\n        \n        source_nostop = []\n        for word in source:\n            if word not in stopwords.words():\n                source_nostop.append(word)\n                \n        lemmatizer = WordNetLemmatizer()\n        source_lem = []\n        for word in source_nostop:\n            source_lem.append(lemmatizer.lemmatize(word))\n        \n        source = \" \".join(source_lem)\n        clean_source[i] = source\n    \n    return clean_source\n\ndef expand_order(row):\n    cell_ids = row[1].split(\" \")\n    df = pd.DataFrame(\n        {\n            \"id\": [row[0] for _ in range(len(cell_ids))],\n            \"cell_id\": cell_ids,\n            \"rank\": range(len(cell_ids)),\n        }\n    )\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-09-02T05:41:21.349917Z","iopub.execute_input":"2022-09-02T05:41:21.350404Z","iopub.status.idle":"2022-09-02T05:41:21.363264Z","shell.execute_reply.started":"2022-09-02T05:41:21.350365Z","shell.execute_reply":"2022-09-02T05:41:21.362138Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def get_input_ids(df, notebook_id, maxlen, tokenizer):\n    # get lists of code_ids, input_ids (from tokenizer), source - all separated based on code/markdown\n    df_temp = df.loc[df['id'] == notebook_id]\n    \n    code = df_temp[df_temp['cell_type'] == 'code']\n    code_ids = code['cell_id'].values.tolist() \n    code_sources = code['clean_source'].values.tolist()  \n    \n    code_encoded = tokenizer.batch_encode_plus(code_sources, add_special_tokens=True, \n                                               return_token_type_ids=True, truncation = True,\n                                               padding='max_length', max_length=maxlen)\n    \n    markdown = df_temp[df_temp['cell_type'] == 'markdown']\n    markdown_ids = markdown['cell_id'].values.tolist()   \n    markdown_sources = markdown['clean_source'].values.tolist()   \n    \n    markdown_encoded = tokenizer.batch_encode_plus(markdown_sources, add_special_tokens=True, \n                                               return_token_type_ids=True, truncation = True,\n                                               padding='max_length', max_length=maxlen)\n    \n    return (code_ids, code_encoded['input_ids'], code_sources, \n            markdown_ids, markdown_encoded['input_ids'], markdown_sources)\n\n\ndef jaccard_similarity(list1, list2):\n    intersection = len(list(set(list1).intersection(list2)))\n    union = (len(set(list1)) + len(set(list2))) - intersection\n    return float(intersection) / union\n\ndef max_similarity(nb_id, code_cell_ids, code_input_ids, mkdn_cell_ids, mkdn_input_ids):\n    # get markdown-code pair with the highest jaccard similarity (based on input_ids)\n    pairings = []\n    \n    for m_counter in range(len(mkdn_cell_ids)):\n        max_similarity = 0 \n        best_c_id = 0\n        current_m_id = mkdn_cell_ids[m_counter]\n        current_m_ii = mkdn_input_ids[m_counter]\n        \n        for c_counter in range(len(code_cell_ids)):\n            current_c_id = code_cell_ids[c_counter]\n            current_c_ii = code_input_ids[c_counter]\n            \n            temp_similarity = jaccard_similarity(current_m_ii, current_c_ii)\n            if temp_similarity > max_similarity:\n                max_similarity = temp_similarity \n                best_c_id = current_c_id\n            \n        pairings.append([current_m_id, best_c_id])\n    \n    return pairings","metadata":{"execution":{"iopub.status.busy":"2022-09-02T05:41:21.366868Z","iopub.execute_input":"2022-09-02T05:41:21.367398Z","iopub.status.idle":"2022-09-02T05:41:21.379446Z","shell.execute_reply.started":"2022-09-02T05:41:21.367362Z","shell.execute_reply":"2022-09-02T05:41:21.378262Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def get_closest_code_rank(m_rank, code_ranks):  \n    min_dis = 0 \n    closest_rank = 0 \n    \n    code_ranks_sorted = copy.deepcopy(code_ranks)\n    code_ranks_sorted.sort()\n\n    for i, c_rank in enumerate(code_ranks_sorted):\n        distance = abs(m_rank - c_rank)\n        prev_distance = abs(m_rank - code_ranks[i-1])\n        if i == 0:\n            min_dis = distance\n            closest_rank = c_rank\n        else:\n            if (distance < min_dis) and (c_rank < m_rank):\n                min_dis = distance\n                closest_rank = c_rank\n    \n    return closest_rank\n\ndef determine_labels(df, notebook_ids):\n    labels = []\n    for nb_id in notebook_ids:\n        df_temp = df.loc[df['id'] == nb_id]\n        \n        # get ranks for code and markdown \n        code_ranks = df_temp.loc[df['cell_type'] == 'code']['rank'].values.tolist()\n        markdown_ranks = df_temp.loc[df['cell_type'] == 'markdown']['rank'].values.tolist()\n        \n        \n        # get cumcounts for code and markdown\n        code_count = df_temp.loc[df['cell_type'] == 'code']['cum_count'].values.tolist()\n        markdown_count = df_temp.loc[df['cell_type'] == 'markdown']['cum_count'].values.tolist()\n        \n        # get max cumcounts for code and markdown \n        max_code_count = max(code_count)\n        max_markdown_count = max(markdown_count)\n         \n        # calculate code labels \n        code_labels = [(count+1)/(max_code_count+2) for count in code_count]\n        code_rank_labels = {code_ranks[i]: code_labels[i] for i in range(len(code_ranks))}\n        \n        # get min code labels \n        min_code_label = min(code_labels)\n        \n        # calculate markdown labels \n        markdown_labels = []\n        for m_rank in markdown_ranks:\n            closest_c_rank = get_closest_code_rank(m_rank, code_ranks)\n            base = (abs(m_rank - closest_c_rank))*(min_code_label/(max_markdown_count+2))\n\n            if m_rank == 0:\n                markdown_labels.append(base)\n                \n            else:\n                closest_code_label = code_rank_labels[closest_c_rank]\n                m_label = closest_code_label + base\n                markdown_labels.append(m_label)\n                \n        labels += code_labels \n        labels += markdown_labels \n        \n    df['labels'] = labels ","metadata":{"execution":{"iopub.status.busy":"2022-09-02T05:41:21.381077Z","iopub.execute_input":"2022-09-02T05:41:21.381897Z","iopub.status.idle":"2022-09-02T05:41:21.396939Z","shell.execute_reply.started":"2022-09-02T05:41:21.381861Z","shell.execute_reply":"2022-09-02T05:41:21.395987Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def get_inputs(df, notebook_ids, pairings, maxlen):   \n    markdown_count = []\n    code_count = []\n    code_label = []\n    base_label = []\n    \n    groups = []\n    \n    input_ids = []\n    attention_mask = []\n    token_type_ids = []\n    \n    for nb_id in tqdm(notebook_ids):\n        pairs = pairings[nb_id]\n        for pair in pairs:\n            # dataframe inputs\n            \n            markdown_id = pair[0]\n            code_id = pair[1]\n            \n            m_count = df.loc[df['cell_id'] == markdown_id]['cum_count'].values.tolist()\n            c_count = df.loc[df['cell_id'] == code_id]['cum_count'].values.tolist()\n            c_label = df.loc[df['cell_id'] == code_id]['labels'].values.tolist()\n            group = df.loc[df['cell_id'] == code_id]['ancestor_id'].values.tolist()\n            b_label = df.loc[(df['id'] == nb_id) & (df['cum_count'] == 0)]['labels'].values.tolist()\n            \n            markdown_count.append(m_count[0])\n            code_count.append(c_count[0])\n            code_label.append(c_label[0])\n            groups.append(group[0])\n            base_label.append(b_label[0])\n            \n            # bert inputs\n            \n            m_source = df.loc[df['cell_id'] == markdown_id]['clean_source'].values.tolist()\n            c_source = df.loc[df['cell_id'] == code_id]['clean_source'].values.tolist()\n            \n            encoding = tokenizer.encode_plus([m_source[0], c_source[0]], add_special_tokens=True, \n                                               return_token_type_ids=True, truncation = True,\n                                               padding='max_length', max_length=maxlen)\n            \n            input_ids.append(encoding['input_ids'])\n            attention_mask.append(encoding['attention_mask'])\n            token_type_ids.append(encoding['token_type_ids'])\n    \n    return (input_ids, attention_mask, token_type_ids, markdown_count, code_count, code_label, base_label, groups)\n\ndef get_labels(df, notebook_ids, pairings):\n    labels = []\n    \n    for nb_id in notebook_ids:\n        pairs = pairings[nb_id]\n        for pair in pairs:\n            markdown_id = pair[0]\n            \n            temp_m_out = df.loc[df['cell_id'] == markdown_id]['labels'].values.tolist()\n            \n            labels.append(temp_m_out[0])\n    \n    return labels","metadata":{"execution":{"iopub.status.busy":"2022-09-02T05:41:21.398593Z","iopub.execute_input":"2022-09-02T05:41:21.399136Z","iopub.status.idle":"2022-09-02T05:41:21.414710Z","shell.execute_reply.started":"2022-09-02T05:41:21.399100Z","shell.execute_reply":"2022-09-02T05:41:21.413764Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def get_dataset(input_ids, attention_mask,  \n                markdown_count, code_count, code_label, base_label, \n                labels = None, ordered = False, repeated = False):\n    # input_ids, attention_mask, token_type_ids\n    # the label_rank for the first code cell in that notebook\n    # the label_rank for the code cell in the pairing\n    \n    if labels is not None:\n        dataset = tf.data.Dataset.from_tensor_slices(\n            ({\"input_ids\": input_ids, \"attention_mask\": attention_mask, \n              \"markdown_count\": markdown_count, \"code_count\": code_count, \n             \"code_label\": code_label, \"base_label\": base_label}, labels)\n        )\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices(\n            {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \n              \"markdown_count\": markdown_count, \"code_count\": code_count, \n             \"code_label\": code_label, \"base_label\": base_label}\n        )\n        \n    if repeated:\n        dataset = dataset.repeat()\n    if not ordered:\n        dataset = dataset.shuffle(1024)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2022-09-02T05:41:21.416362Z","iopub.execute_input":"2022-09-02T05:41:21.416729Z","iopub.status.idle":"2022-09-02T05:41:21.428830Z","shell.execute_reply.started":"2022-09-02T05:41:21.416694Z","shell.execute_reply":"2022-09-02T05:41:21.427645Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def get_model(): \n    backbone = transformers.TFDistilBertModel.from_pretrained(BASE_MODEL)\n    \n    input_ids = tf.keras.layers.Input(\n        shape=(SEQ_LEN,),\n        dtype=tf.int32,\n        name=\"input_ids\",\n    )\n    attention_mask = tf.keras.layers.Input(\n        shape=(SEQ_LEN,),\n        dtype=tf.int32,\n        name=\"attention_mask\",\n    )\n    \n    markdown_count = tf.keras.layers.Input(shape=(1, ), name=\"markdown_count\")\n    code_count = tf.keras.layers.Input(shape=(1, ), name=\"code_count\")\n    code_label = tf.keras.layers.Input(shape=(1, ), name=\"code_label\")\n    base_label = tf.keras.layers.Input(shape=(1, ), name=\"base_label\")\n    \n    \n    concat = tf.keras.layers.Concatenate()([markdown_count, code_count, code_label, base_label])\n    \n    x = backbone(\n        {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask\n        },\n    )\n    \n    x = tf.keras.layers.Dense(32, activation=\"linear\", dtype=\"float32\")(x[0][:, 0, :])\n    \n    y = tf.keras.layers.Dense(32)(concat)\n    \n    xy = tf.keras.layers.Concatenate()([x, y])\n    \n    outputs = tf.keras.layers.Dense(1, activation='linear', dtype='float32')(xy)\n\n    model = tf.keras.Model(\n        inputs=[input_ids, attention_mask, markdown_count, code_count, code_label, base_label],\n        outputs=outputs,\n    )\n    \n    model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n    loss=tf.keras.losses.MeanSquaredError()\n    )\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-09-02T05:41:21.430088Z","iopub.execute_input":"2022-09-02T05:41:21.432079Z","iopub.status.idle":"2022-09-02T05:41:21.444283Z","shell.execute_reply.started":"2022-09-02T05:41:21.432043Z","shell.execute_reply":"2022-09-02T05:41:21.443171Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# CREATE DATASETS","metadata":{}},{"cell_type":"code","source":"df_trial = pd.read_csv(\"../input/clean-data/cleaned_data_.csv\")\ndf_trial = df_trial.dropna()\ndf_trial.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-02T05:41:21.445972Z","iopub.execute_input":"2022-09-02T05:41:21.446342Z","iopub.status.idle":"2022-09-02T05:41:22.080398Z","shell.execute_reply.started":"2022-09-02T05:41:21.446308Z","shell.execute_reply":"2022-09-02T05:41:22.079395Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"notebook_ids = pd.unique(df_trial['id'])\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(BASE_MODEL)\n\ncode_cell_ids = []\ncode_input_ids = []\ncode_sources = {}\n\nmkdn_cell_ids = []\nmkdn_input_ids = []\nmkdn_sources = {}\n\n\nfor nb_id in tqdm(notebook_ids):\n    tci, tce, tcs, tmi, tme, tms = get_input_ids(df_trial, nb_id, SEQ_LEN, tokenizer)\n    code_cell_ids.append(tci) \n    code_input_ids.append(tce) \n    code_sources[nb_id] = tcs\n    \n    mkdn_cell_ids.append(tmi)\n    mkdn_input_ids.append(tme) \n    mkdn_sources[nb_id] = tms","metadata":{"execution":{"iopub.status.busy":"2022-09-02T05:41:22.082059Z","iopub.execute_input":"2022-09-02T05:41:22.082408Z","iopub.status.idle":"2022-09-02T05:41:34.888879Z","shell.execute_reply.started":"2022-09-02T05:41:22.082373Z","shell.execute_reply":"2022-09-02T05:41:34.887825Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"pairings = {}\nfor i in range(len(notebook_ids)):\n    pair = max_similarity(notebook_ids[i], \n                                 code_cell_ids[i], code_input_ids[i], \n                                 mkdn_cell_ids[i], mkdn_input_ids[i])\n    \n    pairings[notebook_ids[i]] = pair","metadata":{"execution":{"iopub.status.busy":"2022-09-02T05:41:34.890250Z","iopub.execute_input":"2022-09-02T05:41:34.891071Z","iopub.status.idle":"2022-09-02T05:41:42.705580Z","shell.execute_reply.started":"2022-09-02T05:41:34.891031Z","shell.execute_reply":"2022-09-02T05:41:42.704518Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df_trial['cum_count'] = df_trial.groupby(['id', 'cell_type']).cumcount()","metadata":{"execution":{"iopub.status.busy":"2022-09-02T05:41:42.706863Z","iopub.execute_input":"2022-09-02T05:41:42.707246Z","iopub.status.idle":"2022-09-02T05:41:42.736407Z","shell.execute_reply.started":"2022-09-02T05:41:42.707208Z","shell.execute_reply":"2022-09-02T05:41:42.735525Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"determine_labels(df_trial, notebook_ids)","metadata":{"execution":{"iopub.status.busy":"2022-09-02T05:41:42.741584Z","iopub.execute_input":"2022-09-02T05:41:42.741889Z","iopub.status.idle":"2022-09-02T05:41:59.484916Z","shell.execute_reply.started":"2022-09-02T05:41:42.741862Z","shell.execute_reply":"2022-09-02T05:41:59.483937Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"input_ids, attention_mask, token_ids, markdown_count, code_count, code_label, base_label, groups = get_inputs(\n    df_trial, notebook_ids, pairings, SEQ_LEN)","metadata":{"execution":{"iopub.status.busy":"2022-09-02T05:41:59.486544Z","iopub.execute_input":"2022-09-02T05:41:59.486941Z","iopub.status.idle":"2022-09-02T05:47:58.147590Z","shell.execute_reply.started":"2022-09-02T05:41:59.486902Z","shell.execute_reply":"2022-09-02T05:47:58.146432Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"labels = get_labels(df_trial, notebook_ids, pairings)","metadata":{"execution":{"iopub.status.busy":"2022-09-02T05:47:58.149381Z","iopub.execute_input":"2022-09-02T05:47:58.150163Z","iopub.status.idle":"2022-09-02T05:48:47.238835Z","shell.execute_reply.started":"2022-09-02T05:47:58.150124Z","shell.execute_reply":"2022-09-02T05:48:47.237828Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"input_ids_np = np.asarray(input_ids)\nattention_mask_np = np.asarray(attention_mask)\ntoken_ids_np = np.asarray(token_ids)\nmarkdown_count_np = np.asarray(markdown_count)\ncode_count_np = np.asarray(code_count)\ncode_label_np = np.asarray(code_label)\nbase_label_np = np.asarray(base_label)\nlabels_np = np.asarray(labels)\ngroups_np = np.asarray(groups)","metadata":{"execution":{"iopub.status.busy":"2022-09-02T05:48:47.240190Z","iopub.execute_input":"2022-09-02T05:48:47.240591Z","iopub.status.idle":"2022-09-02T05:48:47.668049Z","shell.execute_reply.started":"2022-09-02T05:48:47.240555Z","shell.execute_reply":"2022-09-02T05:48:47.667065Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"input_ids_np, attention_mask_np, token_ids_np, markdown_count_np, code_count_np, \\\ncode_label_np, base_label_np, labels_np, groups_np = shuffle(\n    input_ids_np, attention_mask_np, token_ids_np, markdown_count_np, \n    code_count_np, code_label_np, base_label_np, labels_np, groups_np, random_state=RANDOM_STATE)","metadata":{"execution":{"iopub.status.busy":"2022-09-02T05:48:47.669629Z","iopub.execute_input":"2022-09-02T05:48:47.670000Z","iopub.status.idle":"2022-09-02T05:48:47.700754Z","shell.execute_reply.started":"2022-09-02T05:48:47.669963Z","shell.execute_reply":"2022-09-02T05:48:47.699756Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# MODEL TRAINING ","metadata":{}},{"cell_type":"code","source":"if TPU is not None:\n    tf.tpu.experimental.initialize_tpu_system(TPU)\n\n    with STRATEGY.scope():\n        model = get_model()\n        model.summary()\n        \nelse:\n    model = get_model()\n    model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-09-02T05:48:47.702092Z","iopub.execute_input":"2022-09-02T05:48:47.702716Z","iopub.status.idle":"2022-09-02T05:49:08.500734Z","shell.execute_reply.started":"2022-09-02T05:48:47.702676Z","shell.execute_reply":"2022-09-02T05:49:08.498695Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"kfold = GroupKFold(n_splits=N_SPLITS)\n\nfor i, (train_index, val_index) in enumerate(kfold.split(input_ids, labels, groups=groups)):\n    print(i)\n    train_dataset = get_dataset(input_ids_np[train_index], \n                                attention_mask_np[train_index],\n \n                                markdown_count_np[train_index],\n                                code_count_np[train_index],\n                                code_label_np[train_index],\n                                base_label_np[train_index],\n                                labels_np[train_index],\n                                repeated=False)\n    \n    val_dataset = get_dataset(input_ids_np[val_index], \n                              attention_mask_np[val_index],\n\n                              markdown_count_np[val_index],\n                              code_count_np[val_index],\n                              code_label_np[val_index],\n                              base_label_np[val_index],\n                              labels_np[val_index],\n                              ordered=True)\n    \n    print('model training')\n    \n    history = model.fit(train_dataset, validation_data=val_dataset, epochs=10, verbose=1)\n    break","metadata":{"execution":{"iopub.status.busy":"2022-09-02T05:49:08.502314Z","iopub.execute_input":"2022-09-02T05:49:08.502883Z","iopub.status.idle":"2022-09-02T06:10:09.056670Z","shell.execute_reply.started":"2022-09-02T05:49:08.502843Z","shell.execute_reply":"2022-09-02T06:10:09.055683Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"model.save_weights(f\"model_weights.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-09-02T06:14:53.622620Z","iopub.execute_input":"2022-09-02T06:14:53.623028Z","iopub.status.idle":"2022-09-02T06:14:55.140672Z","shell.execute_reply.started":"2022-09-02T06:14:53.622991Z","shell.execute_reply":"2022-09-02T06:14:55.139195Z"},"trusted":true},"execution_count":23,"outputs":[]}]}